import os
import feedparser
import requests
import logging
import time
import re
from datetime import datetime, timedelta
from dotenv import load_dotenv
from urllib.parse import urlparse

# ‡πÇ‡∏´‡∏•‡∏î environment variables
load_dotenv()
WEBHOOK_URL = os.getenv("DISCORD_WEBHOOK_URL")

if not WEBHOOK_URL:
    raise ValueError("‚ùå ‡πÑ‡∏°‡πà‡∏û‡∏ö DISCORD_WEBHOOK_URL ‡πÉ‡∏ô .env")

logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

RSS_FEEDS = [
    "https://www.thairath.co.th/rss/news",
    "https://www.dailynews.co.th/rss/feed/dn/feed.xml",
    "https://www.khaosod.co.th/rss.xml",
    "https://www.matichon.co.th/feed",
    "https://www.naewna.com/rss",
    "https://news.sanook.com/rss/",
    "https://www.prachachat.net/news-rss",
    "https://workpointnews.com/rss",
    "https://www.bangkokpost.com/rss/",
    "https://doh.go.th/rss/newsum",
    "https://www.nationthailand.com/rss",
    "https://rss.app/feeds/AZuuiVkf1deAllDc.xml",
    "https://www.tmd.go.th/api/xml/region-daily-forecast?regionid=7",
    "https://news.google.com/rss/search?q=‡∏ñ‡∏ô‡∏ô‡∏û‡∏£‡∏∞‡∏£‡∏≤‡∏°2&hl=th&gl=TH&ceid=TH:th",
    "https://news.google.com/rss/search?q=‡∏≠‡∏±‡∏°‡∏£‡∏¥‡∏ô‡∏ó‡∏£‡πå‡∏ó‡∏µ‡∏ß‡∏µ&hl=th&gl=TH&ceid=TH:th",
    "https://news.google.com/rss/search?q=‡∏à‡∏£‡∏≤‡∏à‡∏£+‡∏û‡∏£‡∏∞‡∏£‡∏≤‡∏°2&hl=th&gl=TH&ceid=TH:th",
    "https://news.google.com/rss/search?q=js100+‡∏û‡∏£‡∏∞‡∏£‡∏≤‡∏°2&hl=th&gl=TH&ceid=TH:th",
    "https://news.google.com/rss/search?q=‡∏≠‡∏∏‡∏ö‡∏±‡∏ï‡∏¥‡πÄ‡∏´‡∏ï‡∏∏+‡∏û‡∏£‡∏∞‡∏£‡∏≤‡∏°2&hl=th&gl=TH&ceid=TH:th",
    "https://news.google.com/rss/search?q=‡∏ô‡πâ‡∏≥‡∏ó‡πà‡∏ß‡∏°+‡∏û‡∏£‡∏∞‡∏£‡∏≤‡∏°2&hl=th&gl=TH&ceid=TH:th",
]

# ‡∏Ñ‡∏≥‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡∏ó‡∏µ‡πà‡πÉ‡∏ä‡πâ regex ‡∏¢‡∏∑‡∏î‡∏´‡∏¢‡∏∏‡πà‡∏ô‡∏Ç‡∏∂‡πâ‡∏ô
KEYWORDS_REGEX = [
    re.compile(r'‡∏û‡∏£‡∏∞‡∏£‡∏≤‡∏°\s*2'),
    re.compile(r'‡∏ñ‡∏ô‡∏ô\s*‡∏û‡∏£‡∏∞‡∏£‡∏≤‡∏°\s*2'),
    re.compile(r'‡∏ô‡πâ‡∏≥‡∏ó‡πà‡∏ß‡∏°\s*‡∏û‡∏£‡∏∞‡∏£‡∏≤‡∏°\s*2'),
    re.compile(r'‡∏ù‡∏ô‡∏ï‡∏Å\s*‡∏û‡∏£‡∏∞‡∏£‡∏≤‡∏°\s*2'),
    re.compile(r'‡∏≠‡∏∏‡∏ö‡∏±‡∏ï‡∏¥‡πÄ‡∏´‡∏ï‡∏∏\s*‡∏û‡∏£‡∏∞‡∏£‡∏≤‡∏°\s*2'),
    
    re.compile(r'‡∏ô‡πâ‡∏≥‡∏ó‡πà‡∏ß‡∏°'), #test
    re.compile(r'‡∏≠‡∏∏‡∏ö‡∏±‡∏ï‡∏¥‡πÄ‡∏´‡∏ï‡∏∏'), #test
    re.compile(r'‡∏ù‡∏ô‡∏ï‡∏Å') #test
]

SEEN_LINKS_FILE = "seen_links.txt"

def load_seen_links():
    if not os.path.exists(SEEN_LINKS_FILE):
        return set()
    with open(SEEN_LINKS_FILE, "r", encoding="utf-8") as file:
        return set(line.strip() for line in file.readlines())

def save_seen_links(seen_links):
    MAX_LINKS = 1000
    trimmed_links = list(seen_links)[-MAX_LINKS:]
    with open(SEEN_LINKS_FILE, "w", encoding="utf-8") as file:
        file.write("\n".join(trimmed_links))

def escape_discord_markdown(text):
    return re.sub(r'([*_~`[\]])', r'\\\1', text)

def extract_source(url):
    return urlparse(url).netloc.replace("www.", "")

def extract_image_url(entry):
    if "media_content" in entry and entry.media_content:
        return entry.media_content[0].get("url")
    elif "image" in entry:
        return entry.image.get("href")
    return None

def send_discord_notification(title, link, image_url=None):
    source = extract_source(link)
    safe_title = escape_discord_markdown(title)

    embed = {
        "title": safe_title,
        "url": link,
        "color": 0x00b0f4,
        "description": f"**‡πÅ‡∏´‡∏•‡πà‡∏á‡∏Ç‡πà‡∏≤‡∏ß**: {source}",
        "footer": {"text": "‡πÅ‡∏à‡πâ‡∏á‡πÄ‡∏ï‡∏∑‡∏≠‡∏ô‡∏à‡∏≤‡∏Å‡∏£‡∏∞‡∏ö‡∏ö RSS"},
        "timestamp": datetime.utcnow().isoformat()
    }

    if image_url:
        embed["image"] = {"url": image_url}

    payload = {"embeds": [embed]}

    try:
        response = requests.post(WEBHOOK_URL, json=payload)
        response.raise_for_status()
        logging.info(f"‚úÖ ‡∏™‡πà‡∏á‡πÅ‡∏à‡πâ‡∏á‡πÄ‡∏ï‡∏∑‡∏≠‡∏ô‡πÅ‡∏•‡πâ‡∏ß: {title}")
    except requests.RequestException as e:
        logging.error(f"‚ùå ‡∏™‡πà‡∏á webhook ‡πÑ‡∏°‡πà‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à: {e}")

def main():
    logging.info(f"‚úÖ‚úÖ‚úÖ Start-Time : {datetime.now()}")
    seen_links = load_seen_links()
    updated_links = set()
    cutoff_time = datetime.now() - timedelta(days=10)
    # logging.info(f"‚úÖ‚è±Ô∏è‚è±Ô∏è Time : {datetime.now()}")


    for url in RSS_FEEDS:
        try:
            feed = feedparser.parse(url)
            for entry in feed.entries:
                title = entry.title
                description = entry.get("description", "")
                content = f"{title} {description}".lower()
                entry_link = entry.link

                published = entry.get("published_parsed") or entry.get("updated_parsed")
                if published:
                    published_dt = datetime.fromtimestamp(time.mktime(published))
                    if published_dt < cutoff_time:
                        continue

                if entry_link in seen_links:
                    continue

                if any(regex.search(content) for regex in KEYWORDS_REGEX):
                    image_url = extract_image_url(entry)
                    send_discord_notification(title, entry_link, image_url)
                    updated_links.add(entry_link)

        except Exception as e:
            logging.error(f"‚ö†Ô∏è ‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î‡∏Å‡∏±‡∏ö RSS feed: {url} - {e}")

    if updated_links:
        seen_links.update(updated_links)
        save_seen_links(seen_links)
        logging.info(f"üìù ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏•‡∏¥‡∏á‡∏Å‡πå‡πÉ‡∏´‡∏°‡πà‡∏à‡∏≥‡∏ô‡∏ß‡∏ô {len(updated_links)} ‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£‡πÅ‡∏•‡πâ‡∏ß")
        logging.info(f"‚úÖ‚úÖ‚úÖ End-Process : {datetime.now()}")

    else:
        logging.info("üì≠ ‡πÑ‡∏°‡πà‡∏û‡∏ö‡∏Ç‡πà‡∏≤‡∏ß‡πÉ‡∏´‡∏°‡πà‡∏ó‡∏µ‡πà‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Ç‡πâ‡∏≠‡∏á")
        logging.info(f"‚úÖ‚úÖ‚úÖ End-Process : {datetime.now()}")

if __name__ == "__main__":
    try:
        main()
    except Exception as e:
        logging.exception(f"‚ÄºÔ∏è ‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î‡πÉ‡∏ô main(): {e}")
